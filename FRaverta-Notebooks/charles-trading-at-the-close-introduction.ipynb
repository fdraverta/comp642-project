{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available, using CPU instead\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import tensorflow_probability as tfp\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightgbm as lgb\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyError)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "TRAIN_END_DATE = 424\n",
    "df = pd.read_csv('../archive/optiver2023/train.csv')\n",
    "train_df = df[df['date_id'] <= TRAIN_END_DATE]\n",
    "test_df = df[df['date_id'] > TRAIN_END_DATE]\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T05:54:41.523506Z",
     "start_time": "2024-04-05T05:54:41.518529Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "8aaea906",
   "metadata": {
    "papermill": {
     "duration": 0.005628,
     "end_time": "2023-10-20T08:51:22.113230",
     "exception": false,
     "start_time": "2023-10-20T08:51:22.107602",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5b273ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T08:51:22.125385Z",
     "iopub.status.busy": "2023-10-20T08:51:22.125007Z",
     "iopub.status.idle": "2023-10-20T08:51:22.180066Z",
     "shell.execute_reply": "2023-10-20T08:51:22.178671Z"
    },
    "papermill": {
     "duration": 0.06439,
     "end_time": "2023-10-20T08:51:22.182903",
     "exception": false,
     "start_time": "2023-10-20T08:51:22.118513",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-05T04:07:56.278449Z",
     "start_time": "2024-04-05T04:07:56.178948Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "class CFG:\n",
    "    BATCH_SIZE = 32\n",
    "    N_EPOCHS = 500\n",
    "    LEARNING_RATE = 0.0001\n",
    "    N_FOLDS = 5\n",
    "    TARGET_COLS = ['target']\n",
    "    SEED = 2023\n",
    "    N_ASSETS = train_df['stock_id'].nunique()\n",
    "    SCALER = MinMaxScaler()\n",
    "    WEIGHT_DECAY = 0.4\n",
    "    DEBUG = True\n",
    "\n",
    "CFG = CFG()\n",
    "set_seeds(CFG.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4545189",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T08:51:22.240626Z",
     "iopub.status.busy": "2023-10-20T08:51:22.240238Z",
     "iopub.status.idle": "2023-10-20T08:51:22.609620Z",
     "shell.execute_reply": "2023-10-20T08:51:22.608316Z"
    },
    "papermill": {
     "duration": 0.378527,
     "end_time": "2023-10-20T08:51:22.611921",
     "exception": false,
     "start_time": "2023-10-20T08:51:22.233394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock_id                         0\n",
       "date_id                          0\n",
       "seconds_in_bucket                0\n",
       "imbalance_size                  55\n",
       "imbalance_buy_sell_flag          0\n",
       "reference_price                 55\n",
       "matched_size                    55\n",
       "far_price                  1197578\n",
       "near_price                 1179955\n",
       "bid_price                       55\n",
       "bid_size                         0\n",
       "ask_price                       55\n",
       "ask_size                         0\n",
       "wap                             55\n",
       "target                          31\n",
       "time_id                          0\n",
       "row_id                           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf6e8f0",
   "metadata": {
    "papermill": {
     "duration": 0.049812,
     "end_time": "2023-10-20T08:56:45.107512",
     "exception": false,
     "start_time": "2023-10-20T08:56:45.057700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d9b687f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T08:56:45.209163Z",
     "iopub.status.busy": "2023-10-20T08:56:45.208702Z",
     "iopub.status.idle": "2023-10-20T08:56:45.236821Z",
     "shell.execute_reply": "2023-10-20T08:56:45.235529Z"
    },
    "papermill": {
     "duration": 0.081989,
     "end_time": "2023-10-20T08:56:45.239011",
     "exception": false,
     "start_time": "2023-10-20T08:56:45.157022",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-05T05:18:41.389231Z",
     "start_time": "2024-04-05T05:18:41.376674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n0               0        0                  0      3180602.69   \n1               1        0                  0       166603.91   \n2               2        0                  0       302879.87   \n3               3        0                  0     11917682.27   \n4               4        0                  0       447549.96   \n...           ...      ...                ...             ...   \n4621975       195      424                540      1755710.13   \n4621976       196      424                540       325741.25   \n4621977       197      424                540       572162.23   \n4621978       198      424                540            0.00   \n4621979       199      424                540            0.00   \n\n         imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n0                              1         0.999812   13380276.64        NaN   \n1                             -1         0.999896    1642214.25        NaN   \n2                             -1         0.999561    1819368.03        NaN   \n3                             -1         1.000171   18389745.62        NaN   \n4                             -1         0.999532   17860614.95        NaN   \n...                          ...              ...           ...        ...   \n4621975                       -1         1.000637   23963918.01   0.999999   \n4621976                       -1         0.999755    8239249.46   0.999262   \n4621977                        1         0.999981    9207011.59   1.000203   \n4621978                        0         1.001219   72541114.32   1.001219   \n4621979                        0         0.999846   13423224.24   0.999846   \n\n         near_price  bid_price   bid_size  ask_price   ask_size       wap  \\\n0               NaN   0.999812   60651.50   1.000026    8493.03  1.000000   \n1               NaN   0.999896    3233.04   1.000660   20605.09  1.000000   \n2               NaN   0.999403   37956.00   1.000298   18995.00  1.000000   \n3               NaN   0.999999    2324.90   1.000214  479032.40  1.000000   \n4               NaN   0.999394   16485.54   1.000016     434.10  1.000000   \n...             ...        ...        ...        ...        ...       ...   \n4621975    0.999999   1.000637   38626.10   1.000743  185519.18  1.000655   \n4621976    0.999755   0.999755   42180.16   1.000002   16034.04  0.999934   \n4621977    1.000203   0.999870   40522.50   0.999981   61781.16  0.999914   \n4621978    1.001219   1.000953  497503.65   1.001219  630439.20  1.001070   \n4621979    0.999846   0.999846  115633.88   1.000188  205235.56  0.999969   \n\n           target  time_id       row_id  \n0       -3.029704        0        0_0_0  \n1       -5.519986        0        0_0_1  \n2       -8.389950        0        0_0_2  \n3       -4.010200        0        0_0_3  \n4       -7.349849        0        0_0_4  \n...           ...      ...          ...  \n4621975  2.089739    23374  424_540_195  \n4621976  2.579689    23374  424_540_196  \n4621977 -2.359748    23374  424_540_197  \n4621978  9.089708    23374  424_540_198  \n4621979  2.570152    23374  424_540_199  \n\n[4621980 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stock_id</th>\n      <th>date_id</th>\n      <th>seconds_in_bucket</th>\n      <th>imbalance_size</th>\n      <th>imbalance_buy_sell_flag</th>\n      <th>reference_price</th>\n      <th>matched_size</th>\n      <th>far_price</th>\n      <th>near_price</th>\n      <th>bid_price</th>\n      <th>bid_size</th>\n      <th>ask_price</th>\n      <th>ask_size</th>\n      <th>wap</th>\n      <th>target</th>\n      <th>time_id</th>\n      <th>row_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3180602.69</td>\n      <td>1</td>\n      <td>0.999812</td>\n      <td>13380276.64</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999812</td>\n      <td>60651.50</td>\n      <td>1.000026</td>\n      <td>8493.03</td>\n      <td>1.000000</td>\n      <td>-3.029704</td>\n      <td>0</td>\n      <td>0_0_0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>166603.91</td>\n      <td>-1</td>\n      <td>0.999896</td>\n      <td>1642214.25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999896</td>\n      <td>3233.04</td>\n      <td>1.000660</td>\n      <td>20605.09</td>\n      <td>1.000000</td>\n      <td>-5.519986</td>\n      <td>0</td>\n      <td>0_0_1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>302879.87</td>\n      <td>-1</td>\n      <td>0.999561</td>\n      <td>1819368.03</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999403</td>\n      <td>37956.00</td>\n      <td>1.000298</td>\n      <td>18995.00</td>\n      <td>1.000000</td>\n      <td>-8.389950</td>\n      <td>0</td>\n      <td>0_0_2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11917682.27</td>\n      <td>-1</td>\n      <td>1.000171</td>\n      <td>18389745.62</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999999</td>\n      <td>2324.90</td>\n      <td>1.000214</td>\n      <td>479032.40</td>\n      <td>1.000000</td>\n      <td>-4.010200</td>\n      <td>0</td>\n      <td>0_0_3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>447549.96</td>\n      <td>-1</td>\n      <td>0.999532</td>\n      <td>17860614.95</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999394</td>\n      <td>16485.54</td>\n      <td>1.000016</td>\n      <td>434.10</td>\n      <td>1.000000</td>\n      <td>-7.349849</td>\n      <td>0</td>\n      <td>0_0_4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4621975</th>\n      <td>195</td>\n      <td>424</td>\n      <td>540</td>\n      <td>1755710.13</td>\n      <td>-1</td>\n      <td>1.000637</td>\n      <td>23963918.01</td>\n      <td>0.999999</td>\n      <td>0.999999</td>\n      <td>1.000637</td>\n      <td>38626.10</td>\n      <td>1.000743</td>\n      <td>185519.18</td>\n      <td>1.000655</td>\n      <td>2.089739</td>\n      <td>23374</td>\n      <td>424_540_195</td>\n    </tr>\n    <tr>\n      <th>4621976</th>\n      <td>196</td>\n      <td>424</td>\n      <td>540</td>\n      <td>325741.25</td>\n      <td>-1</td>\n      <td>0.999755</td>\n      <td>8239249.46</td>\n      <td>0.999262</td>\n      <td>0.999755</td>\n      <td>0.999755</td>\n      <td>42180.16</td>\n      <td>1.000002</td>\n      <td>16034.04</td>\n      <td>0.999934</td>\n      <td>2.579689</td>\n      <td>23374</td>\n      <td>424_540_196</td>\n    </tr>\n    <tr>\n      <th>4621977</th>\n      <td>197</td>\n      <td>424</td>\n      <td>540</td>\n      <td>572162.23</td>\n      <td>1</td>\n      <td>0.999981</td>\n      <td>9207011.59</td>\n      <td>1.000203</td>\n      <td>1.000203</td>\n      <td>0.999870</td>\n      <td>40522.50</td>\n      <td>0.999981</td>\n      <td>61781.16</td>\n      <td>0.999914</td>\n      <td>-2.359748</td>\n      <td>23374</td>\n      <td>424_540_197</td>\n    </tr>\n    <tr>\n      <th>4621978</th>\n      <td>198</td>\n      <td>424</td>\n      <td>540</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>1.001219</td>\n      <td>72541114.32</td>\n      <td>1.001219</td>\n      <td>1.001219</td>\n      <td>1.000953</td>\n      <td>497503.65</td>\n      <td>1.001219</td>\n      <td>630439.20</td>\n      <td>1.001070</td>\n      <td>9.089708</td>\n      <td>23374</td>\n      <td>424_540_198</td>\n    </tr>\n    <tr>\n      <th>4621979</th>\n      <td>199</td>\n      <td>424</td>\n      <td>540</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.999846</td>\n      <td>13423224.24</td>\n      <td>0.999846</td>\n      <td>0.999846</td>\n      <td>0.999846</td>\n      <td>115633.88</td>\n      <td>1.000188</td>\n      <td>205235.56</td>\n      <td>0.999969</td>\n      <td>2.570152</td>\n      <td>23374</td>\n      <td>424_540_199</td>\n    </tr>\n  </tbody>\n</table>\n<p>4621980 rows × 17 columns</p>\n</div>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "75d45292",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T08:56:45.344668Z",
     "iopub.status.busy": "2023-10-20T08:56:45.344254Z",
     "iopub.status.idle": "2023-10-20T08:57:47.538468Z",
     "shell.execute_reply": "2023-10-20T08:57:47.537226Z"
    },
    "papermill": {
     "duration": 62.250548,
     "end_time": "2023-10-20T08:57:47.540740",
     "exception": false,
     "start_time": "2023-10-20T08:56:45.290192",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-05T05:29:30.808528Z",
     "start_time": "2024-04-05T05:29:19.987513Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'log_return'] = np.log(df['wap'])\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(['imbalance_buy_sell_flag', 'far_price', 'near_price'], axis=1, inplace=True)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'imbalance_size_lag_{i}'] = df.groupby('stock_id')['imbalance_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'reference_price_lag_{i}'] = df.groupby('stock_id')['reference_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'matched_size_lag_{i}'] = df.groupby('stock_id')['matched_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'bid_price_lag_{i}'] = df.groupby('stock_id')['bid_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'ask_price_lag_{i}'] = df.groupby('stock_id')['ask_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'wap_{i}'] = df.groupby('stock_id')['wap'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'imbalance_size_lag_{i}'] = df.groupby('stock_id')['imbalance_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'reference_price_lag_{i}'] = df.groupby('stock_id')['reference_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'matched_size_lag_{i}'] = df.groupby('stock_id')['matched_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'bid_price_lag_{i}'] = df.groupby('stock_id')['bid_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'ask_price_lag_{i}'] = df.groupby('stock_id')['ask_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'wap_{i}'] = df.groupby('stock_id')['wap'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'imbalance_size_lag_{i}'] = df.groupby('stock_id')['imbalance_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'reference_price_lag_{i}'] = df.groupby('stock_id')['reference_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'matched_size_lag_{i}'] = df.groupby('stock_id')['matched_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'bid_price_lag_{i}'] = df.groupby('stock_id')['bid_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'ask_price_lag_{i}'] = df.groupby('stock_id')['ask_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'wap_{i}'] = df.groupby('stock_id')['wap'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'imbalance_size_lag_{i}'] = df.groupby('stock_id')['imbalance_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'reference_price_lag_{i}'] = df.groupby('stock_id')['reference_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'matched_size_lag_{i}'] = df.groupby('stock_id')['matched_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'bid_price_lag_{i}'] = df.groupby('stock_id')['bid_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'ask_price_lag_{i}'] = df.groupby('stock_id')['ask_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'wap_{i}'] = df.groupby('stock_id')['wap'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'imbalance_size_lag_{i}'] = df.groupby('stock_id')['imbalance_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'reference_price_lag_{i}'] = df.groupby('stock_id')['reference_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'matched_size_lag_{i}'] = df.groupby('stock_id')['matched_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'bid_price_lag_{i}'] = df.groupby('stock_id')['bid_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'ask_price_lag_{i}'] = df.groupby('stock_id')['ask_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'wap_{i}'] = df.groupby('stock_id')['wap'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'imbalance_size_lag_{i}'] = df.groupby('stock_id')['imbalance_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'reference_price_lag_{i}'] = df.groupby('stock_id')['reference_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'matched_size_lag_{i}'] = df.groupby('stock_id')['matched_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'bid_price_lag_{i}'] = df.groupby('stock_id')['bid_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'ask_price_lag_{i}'] = df.groupby('stock_id')['ask_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'wap_{i}'] = df.groupby('stock_id')['wap'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'imbalance_size_lag_{i}'] = df.groupby('stock_id')['imbalance_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'reference_price_lag_{i}'] = df.groupby('stock_id')['reference_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'matched_size_lag_{i}'] = df.groupby('stock_id')['matched_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'bid_price_lag_{i}'] = df.groupby('stock_id')['bid_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'ask_price_lag_{i}'] = df.groupby('stock_id')['ask_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'wap_{i}'] = df.groupby('stock_id')['wap'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'imbalance_size_lag_{i}'] = df.groupby('stock_id')['imbalance_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'reference_price_lag_{i}'] = df.groupby('stock_id')['reference_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'matched_size_lag_{i}'] = df.groupby('stock_id')['matched_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'bid_price_lag_{i}'] = df.groupby('stock_id')['bid_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'ask_price_lag_{i}'] = df.groupby('stock_id')['ask_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'wap_{i}'] = df.groupby('stock_id')['wap'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'imbalance_size_lag_{i}'] = df.groupby('stock_id')['imbalance_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'reference_price_lag_{i}'] = df.groupby('stock_id')['reference_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'matched_size_lag_{i}'] = df.groupby('stock_id')['matched_size'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'bid_price_lag_{i}'] = df.groupby('stock_id')['bid_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'ask_price_lag_{i}'] = df.groupby('stock_id')['ask_price'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'wap_{i}'] = df.groupby('stock_id')['wap'].shift(i)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['bid_size_lag_1'] = df.groupby('stock_id')['bid_size'].shift(1)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['ask_size_lag_1'] = df.groupby('stock_id')['ask_size'].shift(1)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.fillna(0, inplace=True)\n",
      "/var/folders/7s/4djnjpn91clbt_kdpqldx_sc0000gn/T/ipykernel_30735/4085314819.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[cols_to_norm] = CFG.SCALER.fit_transform(df[cols_to_norm])\n"
     ]
    }
   ],
   "source": [
    "def preprocess(df, mode='train'):\n",
    "    # print(df.columns)\n",
    "    df['log_return'] = np.log(df['wap'])\n",
    "    df.drop(['imbalance_buy_sell_flag', 'far_price', 'near_price'], axis=1, inplace=True)\n",
    "    \n",
    "    for i in range(1, 10):\n",
    "        df[f'imbalance_size_lag_{i}'] = df.groupby('stock_id')['imbalance_size'].shift(i)\n",
    "        df[f'reference_price_lag_{i}'] = df.groupby('stock_id')['reference_price'].shift(i)\n",
    "        df[f'matched_size_lag_{i}'] = df.groupby('stock_id')['matched_size'].shift(i)\n",
    "        df[f'bid_price_lag_{i}'] = df.groupby('stock_id')['bid_price'].shift(i)\n",
    "        df[f'ask_price_lag_{i}'] = df.groupby('stock_id')['ask_price'].shift(i)\n",
    "        df[f'wap_{i}'] = df.groupby('stock_id')['wap'].shift(i)\n",
    "\n",
    "    df['bid_size_lag_1'] = df.groupby('stock_id')['bid_size'].shift(1)\n",
    "    df['ask_size_lag_1'] = df.groupby('stock_id')['ask_size'].shift(1)\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "    # Standardize\n",
    "    if mode == 'train':\n",
    "        # print(df.columns)\n",
    "        cols_to_norm = df.drop(['stock_id', 'time_id', 'date_id', 'row_id', 'target', 'seconds_in_bucket'], axis=1).columns\n",
    "        df[cols_to_norm] = CFG.SCALER.fit_transform(df[cols_to_norm])\n",
    "    elif mode == 'test':\n",
    "        cols_to_norm = df.drop(['stock_id', 'time_id', 'date_id', 'row_id', 'target', 'seconds_in_bucket'], axis=1).columns\n",
    "        df[cols_to_norm] = CFG.SCALER.transform(df[cols_to_norm])\n",
    "    else:\n",
    "        print('Wrong Mode.')\n",
    "    return df\n",
    "    \n",
    "train_df = preprocess(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b4f6d020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T08:57:47.644136Z",
     "iopub.status.busy": "2023-10-20T08:57:47.643703Z",
     "iopub.status.idle": "2023-10-20T08:57:49.541290Z",
     "shell.execute_reply": "2023-10-20T08:57:49.540203Z"
    },
    "papermill": {
     "duration": 1.952929,
     "end_time": "2023-10-20T08:57:49.543464",
     "exception": false,
     "start_time": "2023-10-20T08:57:47.590535",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-05T05:29:33.343225Z",
     "start_time": "2024-04-05T05:29:33.096333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n0               0        0                  0        0.001067   \n191             0        0                 10        0.000436   \n382             0        0                 20        0.000436   \n573             0        0                 30        0.000436   \n764             0        0                 40        0.000409   \n...           ...      ...                ...             ...   \n4620980         0      424                500        0.001405   \n4621180         0      424                510        0.001399   \n4621380         0      424                520        0.001382   \n4621580         0      424                530        0.001379   \n4621780         0      424                540        0.001370   \n\n         reference_price  matched_size  bid_price  bid_size  ask_price  \\\n0               0.927910      0.001735   0.927910  0.002003   0.927809   \n191             0.928109      0.001978   0.927910  0.000462   0.927809   \n382             0.928009      0.001978   0.927910  0.000154   0.927710   \n573             0.928208      0.001978   0.928109  0.001849   0.927908   \n764             0.928507      0.001989   0.928308  0.000484   0.928207   \n...                  ...           ...        ...       ...        ...   \n4620980         0.929026      0.006887   0.929026  0.003548   0.928903   \n4621180         0.929026      0.006890   0.929026  0.004012   0.928903   \n4621380         0.929026      0.006896   0.929026  0.005256   0.928903   \n4621580         0.929026      0.006898   0.929026  0.006528   0.928903   \n4621780         0.929203      0.006901   0.929203  0.001024   0.929080   \n\n         ask_size  ...  ask_price_lag_8     wap_8  imbalance_size_lag_9  \\\n0        0.000156  ...         0.000000  0.000000              0.000000   \n191      0.000432  ...         0.000000  0.000000              0.000000   \n382      0.000223  ...         0.000000  0.000000              0.000000   \n573      0.000849  ...         0.000000  0.000000              0.000000   \n764      0.000489  ...         0.000000  0.000000              0.000000   \n...           ...  ...              ...       ...                   ...   \n4620980  0.002127  ...         0.928903  0.928956              0.001271   \n4621180  0.002449  ...         0.928903  0.928968              0.001274   \n4621380  0.003862  ...         0.928903  0.928934              0.001271   \n4621580  0.003394  ...         0.928726  0.928715              0.001264   \n4621780  0.002369  ...         0.928726  0.928756              0.001242   \n\n        reference_price_lag_9  matched_size_lag_9  bid_price_lag_9  \\\n0                    0.000000            0.000000         0.000000   \n191                  0.000000            0.000000         0.000000   \n382                  0.000000            0.000000         0.000000   \n573                  0.000000            0.000000         0.000000   \n764                  0.000000            0.000000         0.000000   \n...                       ...                 ...              ...   \n4620980              0.928849            0.006861         0.928849   \n4621180              0.929026            0.006860         0.929026   \n4621380              0.929026            0.006861         0.929026   \n4621580              0.929026            0.006864         0.929026   \n4621780              0.928849            0.006872         0.928849   \n\n         ask_price_lag_9     wap_9  bid_size_lag_1  ask_size_lag_1  \n0               0.000000  0.000000        0.000000        0.000000  \n191             0.000000  0.000000        0.002003        0.000156  \n382             0.000000  0.000000        0.000462        0.000432  \n573             0.000000  0.000000        0.000154        0.000223  \n764             0.000000  0.000000        0.001849        0.000849  \n...                  ...       ...             ...             ...  \n4620980         0.928726  0.928788        0.002133        0.001948  \n4621180         0.928903  0.928956        0.003548        0.002127  \n4621380         0.928903  0.928968        0.004012        0.002449  \n4621580         0.928903  0.928934        0.005256        0.003862  \n4621780         0.928726  0.928715        0.006528        0.003394  \n\n[23375 rows x 71 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stock_id</th>\n      <th>date_id</th>\n      <th>seconds_in_bucket</th>\n      <th>imbalance_size</th>\n      <th>reference_price</th>\n      <th>matched_size</th>\n      <th>bid_price</th>\n      <th>bid_size</th>\n      <th>ask_price</th>\n      <th>ask_size</th>\n      <th>...</th>\n      <th>ask_price_lag_8</th>\n      <th>wap_8</th>\n      <th>imbalance_size_lag_9</th>\n      <th>reference_price_lag_9</th>\n      <th>matched_size_lag_9</th>\n      <th>bid_price_lag_9</th>\n      <th>ask_price_lag_9</th>\n      <th>wap_9</th>\n      <th>bid_size_lag_1</th>\n      <th>ask_size_lag_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.001067</td>\n      <td>0.927910</td>\n      <td>0.001735</td>\n      <td>0.927910</td>\n      <td>0.002003</td>\n      <td>0.927809</td>\n      <td>0.000156</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>191</th>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>0.000436</td>\n      <td>0.928109</td>\n      <td>0.001978</td>\n      <td>0.927910</td>\n      <td>0.000462</td>\n      <td>0.927809</td>\n      <td>0.000432</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.002003</td>\n      <td>0.000156</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>0</td>\n      <td>0</td>\n      <td>20</td>\n      <td>0.000436</td>\n      <td>0.928009</td>\n      <td>0.001978</td>\n      <td>0.927910</td>\n      <td>0.000154</td>\n      <td>0.927710</td>\n      <td>0.000223</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000462</td>\n      <td>0.000432</td>\n    </tr>\n    <tr>\n      <th>573</th>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n      <td>0.000436</td>\n      <td>0.928208</td>\n      <td>0.001978</td>\n      <td>0.928109</td>\n      <td>0.001849</td>\n      <td>0.927908</td>\n      <td>0.000849</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000154</td>\n      <td>0.000223</td>\n    </tr>\n    <tr>\n      <th>764</th>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>0.000409</td>\n      <td>0.928507</td>\n      <td>0.001989</td>\n      <td>0.928308</td>\n      <td>0.000484</td>\n      <td>0.928207</td>\n      <td>0.000489</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.001849</td>\n      <td>0.000849</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4620980</th>\n      <td>0</td>\n      <td>424</td>\n      <td>500</td>\n      <td>0.001405</td>\n      <td>0.929026</td>\n      <td>0.006887</td>\n      <td>0.929026</td>\n      <td>0.003548</td>\n      <td>0.928903</td>\n      <td>0.002127</td>\n      <td>...</td>\n      <td>0.928903</td>\n      <td>0.928956</td>\n      <td>0.001271</td>\n      <td>0.928849</td>\n      <td>0.006861</td>\n      <td>0.928849</td>\n      <td>0.928726</td>\n      <td>0.928788</td>\n      <td>0.002133</td>\n      <td>0.001948</td>\n    </tr>\n    <tr>\n      <th>4621180</th>\n      <td>0</td>\n      <td>424</td>\n      <td>510</td>\n      <td>0.001399</td>\n      <td>0.929026</td>\n      <td>0.006890</td>\n      <td>0.929026</td>\n      <td>0.004012</td>\n      <td>0.928903</td>\n      <td>0.002449</td>\n      <td>...</td>\n      <td>0.928903</td>\n      <td>0.928968</td>\n      <td>0.001274</td>\n      <td>0.929026</td>\n      <td>0.006860</td>\n      <td>0.929026</td>\n      <td>0.928903</td>\n      <td>0.928956</td>\n      <td>0.003548</td>\n      <td>0.002127</td>\n    </tr>\n    <tr>\n      <th>4621380</th>\n      <td>0</td>\n      <td>424</td>\n      <td>520</td>\n      <td>0.001382</td>\n      <td>0.929026</td>\n      <td>0.006896</td>\n      <td>0.929026</td>\n      <td>0.005256</td>\n      <td>0.928903</td>\n      <td>0.003862</td>\n      <td>...</td>\n      <td>0.928903</td>\n      <td>0.928934</td>\n      <td>0.001271</td>\n      <td>0.929026</td>\n      <td>0.006861</td>\n      <td>0.929026</td>\n      <td>0.928903</td>\n      <td>0.928968</td>\n      <td>0.004012</td>\n      <td>0.002449</td>\n    </tr>\n    <tr>\n      <th>4621580</th>\n      <td>0</td>\n      <td>424</td>\n      <td>530</td>\n      <td>0.001379</td>\n      <td>0.929026</td>\n      <td>0.006898</td>\n      <td>0.929026</td>\n      <td>0.006528</td>\n      <td>0.928903</td>\n      <td>0.003394</td>\n      <td>...</td>\n      <td>0.928726</td>\n      <td>0.928715</td>\n      <td>0.001264</td>\n      <td>0.929026</td>\n      <td>0.006864</td>\n      <td>0.929026</td>\n      <td>0.928903</td>\n      <td>0.928934</td>\n      <td>0.005256</td>\n      <td>0.003862</td>\n    </tr>\n    <tr>\n      <th>4621780</th>\n      <td>0</td>\n      <td>424</td>\n      <td>540</td>\n      <td>0.001370</td>\n      <td>0.929203</td>\n      <td>0.006901</td>\n      <td>0.929203</td>\n      <td>0.001024</td>\n      <td>0.929080</td>\n      <td>0.002369</td>\n      <td>...</td>\n      <td>0.928726</td>\n      <td>0.928756</td>\n      <td>0.001242</td>\n      <td>0.928849</td>\n      <td>0.006872</td>\n      <td>0.928849</td>\n      <td>0.928726</td>\n      <td>0.928715</td>\n      <td>0.006528</td>\n      <td>0.003394</td>\n    </tr>\n  </tbody>\n</table>\n<p>23375 rows × 71 columns</p>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['stock_id']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0fac6",
   "metadata": {
    "papermill": {
     "duration": 0.049907,
     "end_time": "2023-10-20T08:57:49.642396",
     "exception": false,
     "start_time": "2023-10-20T08:57:49.592489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "outout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ea9f4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T08:57:49.749313Z",
     "iopub.status.busy": "2023-10-20T08:57:49.748917Z",
     "iopub.status.idle": "2023-10-20T08:57:49.766020Z",
     "shell.execute_reply": "2023-10-20T08:57:49.764931Z"
    },
    "papermill": {
     "duration": 0.075441,
     "end_time": "2023-10-20T08:57:49.768707",
     "exception": false,
     "start_time": "2023-10-20T08:57:49.693266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    stacking_mode : bool, default=True\n",
    "        Whether to provide an additional set to test a stacking classifier or not. \n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    max_val_group_size : int, default=Inf\n",
    "        Maximum group size for a single validation set.\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split, if stacking_mode = True and None \n",
    "        it defaults to max_val_group_size.\n",
    "    val_group_gap : int, default=None\n",
    "        Gap between train and validation\n",
    "    test_group_gap : int, default=None\n",
    "        Gap between validation and test, if stacking_mode = True and None \n",
    "        it defaults to val_group_gap.\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_val_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 val_group_gap=None,\n",
    "                 test_group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.max_val_group_size = max_val_group_size\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.val_group_gap = val_group_gap\n",
    "        self.test_group_gap = test_group_gap\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        return self.split_standard(X, y, groups)\n",
    "        \n",
    "    def split_standard(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and validation set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/validation set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        val : ndarray\n",
    "            The validation set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.val_group_gap\n",
    "        max_val_group_size = self.max_val_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_val_size = min(n_groups // n_folds, max_val_group_size)\n",
    "        group_val_starts = range(n_groups - n_splits * group_val_size,\n",
    "                                  n_groups, group_val_size)\n",
    "        for group_val_start in group_val_starts:\n",
    "            train_array = []\n",
    "            val_array = []\n",
    "\n",
    "            group_st = max(0, group_val_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_val_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for val_group_idx in unique_groups[group_val_start:\n",
    "                                                group_val_start +\n",
    "                                                group_val_size]:\n",
    "                val_array_tmp = group_dict[val_group_idx]\n",
    "                val_array = np.sort(np.unique(\n",
    "                                              np.concatenate((val_array,\n",
    "                                                              val_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            val_array  = val_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in val_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa0ba68",
   "metadata": {
    "papermill": {
     "duration": 0.048541,
     "end_time": "2023-10-20T08:57:49.867272",
     "exception": false,
     "start_time": "2023-10-20T08:57:49.818731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "071d2ed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T08:57:49.967740Z",
     "iopub.status.busy": "2023-10-20T08:57:49.967403Z",
     "iopub.status.idle": "2023-10-20T08:57:49.977278Z",
     "shell.execute_reply": "2023-10-20T08:57:49.975421Z"
    },
    "papermill": {
     "duration": 0.062719,
     "end_time": "2023-10-20T08:57:49.979347",
     "exception": false,
     "start_time": "2023-10-20T08:57:49.916628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def global_average_pooling(x):\n",
    "    return x.mean(dim=(-1))\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.batchnorm1d = nn.BatchNorm1d(200)\n",
    "        self.conv1d1 = nn.Conv1d(200, 8, 3)\n",
    "        self.conv1d2 = nn.Conv1d(8, 8, 3)\n",
    "        self.conv1d3 = nn.Conv1d(8, 16, 3)\n",
    "        self.conv1d4 = nn.Conv1d(16, 16, 3)\n",
    "        self.conv1d5 = nn.Conv1d(16, 32, 3)\n",
    "        self.conv1d6 = nn.Conv1d(32, 32, 3)\n",
    "        self.conv1d7 = nn.Conv1d(32, 32, 3)\n",
    "        \n",
    "        self.pool1d1 = nn.AvgPool1d(2)\n",
    "        self.pool1d2 = nn.AvgPool1d(2)\n",
    "     \n",
    "        self.linear1 = nn.Linear(32, 32)\n",
    "        self.linear2 = nn.Linear(32, 200)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.batchnorm1d(inputs)\n",
    "        x = self.conv1d1(x)\n",
    "        x = self.conv1d2(x)\n",
    "        x = self.pool1d1(x)\n",
    "        \n",
    "        x = self.conv1d3(x)\n",
    "        x = self.conv1d4(x)\n",
    "        x = self.pool1d2(x)\n",
    "        \n",
    "        x = self.conv1d5(x)\n",
    "        x = self.conv1d6(x)\n",
    "        x = self.conv1d7(x)\n",
    "\n",
    "        out = global_average_pooling(x)\n",
    "\n",
    "        out = self.linear1(out)\n",
    "        out = nn.ReLU()(out)\n",
    "        \n",
    "        output = self.linear2(out)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e03f7a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T08:57:50.080631Z",
     "iopub.status.busy": "2023-10-20T08:57:50.079352Z",
     "iopub.status.idle": "2023-10-20T08:57:50.088617Z",
     "shell.execute_reply": "2023-10-20T08:57:50.087293Z"
    },
    "papermill": {
     "duration": 0.062632,
     "end_time": "2023-10-20T08:57:50.091060",
     "exception": false,
     "start_time": "2023-10-20T08:57:50.028428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_df(df):\n",
    "    missing_ids = list(set(np.arange(0, 200)) - set(df.index))\n",
    "    for id in missing_ids:\n",
    "        df.loc[id] = 0\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "    \n",
    "def get_dataset(df):\n",
    "    X = []\n",
    "    y = []\n",
    "    for name, group in df.groupby('time_id'):\n",
    "        group = group.set_index('stock_id')\n",
    "        if len(group) < 200:\n",
    "            group = pad_df(group)  \n",
    "        features = torch.tensor(group.drop(['time_id', 'date_id', 'row_id', 'target'], axis=1).values, dtype=torch.float32)\n",
    "        labels = group['target'].values \n",
    "        X.append(features)\n",
    "        y.append(labels)\n",
    "    X_ = torch.stack(X)\n",
    "    y_ = torch.tensor(np.array(y), dtype=torch.float32)\n",
    "    return TensorDataset(X_, y_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74d6d060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T08:57:50.192897Z",
     "iopub.status.busy": "2023-10-20T08:57:50.192172Z",
     "iopub.status.idle": "2023-10-20T08:57:50.209390Z",
     "shell.execute_reply": "2023-10-20T08:57:50.208079Z"
    },
    "papermill": {
     "duration": 0.071038,
     "end_time": "2023-10-20T08:57:50.211936",
     "exception": false,
     "start_time": "2023-10-20T08:57:50.140898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def get_score(y_val, y_pred):\n",
    "    all_scores = []\n",
    "    for i in range(len(y_val)):\n",
    "        score = mean_absolute_error(y_val[i], y_pred[i])\n",
    "        all_scores.append(score)\n",
    "    return np.mean(all_scores)\n",
    "        \n",
    "def train_fn(fold, train_dataloader, model, loss_fn, optimizer, epoch, device):\n",
    "    losses = AverageMeter()\n",
    "    model.train()\n",
    "    for step, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        losses.update(loss.item(), labels.size(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return losses.avg\n",
    "\n",
    "def valid_fn(valid_dataloader, model, loss_fn, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for step, (input_ids, labels) in enumerate(valid_dataloader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)   \n",
    "            loss = loss_fn(outputs, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(outputs.to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions\n",
    "\n",
    "\n",
    "def train_loop(train, val, fold):\n",
    "    print(f'----------------- Fold: {fold+1} -----------------') \n",
    "    \n",
    "    train_ds = get_dataset(train)\n",
    "    val_ds = get_dataset(val)\n",
    "    _, y_val = val_ds[:]\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "            train_ds,  \n",
    "            batch_size = CFG.BATCH_SIZE \n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "            val_ds,  \n",
    "            batch_size = CFG.BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    model = CNN().to(device)\n",
    "    optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters()],\n",
    "             'lr': CFG.LEARNING_RATE, 'weight_decay': CFG.WEIGHT_DECAY},\n",
    "        ]\n",
    "\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(optimizer_parameters)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    min_score = np.inf        \n",
    "    wait = 0\n",
    "    patience=10\n",
    "    \n",
    "    for epoch in range(CFG.N_EPOCHS):\n",
    "        avg_loss = train_fn(fold, train_dataloader, model, loss_fn, optimizer, epoch, device)\n",
    "\n",
    "        avg_val_loss, y_pred = valid_fn(val_dataloader, model, loss_fn, device)\n",
    "   \n",
    "        score = get_score(y_val, y_pred)\n",
    "\n",
    "        print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f} avg_val_loss: {avg_val_loss:.4f} MAE: {score:.4f}')\n",
    "           \n",
    "        wait += 1\n",
    "        if round(score, 4) < round(min_score, 4):\n",
    "            min_score = score\n",
    "            wait = 0\n",
    "            torch.save(model.state_dict(), f'model_fold_{fold+1}')\n",
    "            print(f'﹂ saving model with score: {min_score:.4f}')\n",
    "        if wait >= patience:\n",
    "            print(f'Triggering Early Stopping on epoch {epoch+1}')\n",
    "            return min_score\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb75bc72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T08:57:50.335357Z",
     "iopub.status.busy": "2023-10-20T08:57:50.334500Z",
     "iopub.status.idle": "2023-10-20T09:35:32.198548Z",
     "shell.execute_reply": "2023-10-20T09:35:32.196931Z"
    },
    "papermill": {
     "duration": 2261.927456,
     "end_time": "2023-10-20T09:35:32.201656",
     "exception": false,
     "start_time": "2023-10-20T08:57:50.274200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Fold: 1 -----------------\n",
      "Epoch 1 - avg_train_loss: 89.2945 avg_val_loss: 93.1047 MAE: 6.8994\n",
      "﹂ saving model with score: 6.8994\n",
      "Epoch 2 - avg_train_loss: 89.2852 avg_val_loss: 93.0827 MAE: 6.8983\n",
      "﹂ saving model with score: 6.8983\n",
      "Epoch 3 - avg_train_loss: 89.2757 avg_val_loss: 93.0598 MAE: 6.8972\n",
      "﹂ saving model with score: 6.8972\n",
      "Epoch 4 - avg_train_loss: 89.2654 avg_val_loss: 93.0403 MAE: 6.8962\n",
      "﹂ saving model with score: 6.8962\n",
      "Epoch 5 - avg_train_loss: 89.2554 avg_val_loss: 93.0233 MAE: 6.8953\n",
      "﹂ saving model with score: 6.8953\n",
      "Epoch 6 - avg_train_loss: 89.2464 avg_val_loss: 93.0088 MAE: 6.8945\n",
      "﹂ saving model with score: 6.8945\n",
      "Epoch 7 - avg_train_loss: 89.2384 avg_val_loss: 92.9966 MAE: 6.8938\n",
      "﹂ saving model with score: 6.8938\n",
      "Epoch 8 - avg_train_loss: 89.2316 avg_val_loss: 92.9868 MAE: 6.8932\n",
      "﹂ saving model with score: 6.8932\n",
      "Epoch 9 - avg_train_loss: 89.2256 avg_val_loss: 92.9787 MAE: 6.8927\n",
      "﹂ saving model with score: 6.8927\n",
      "Epoch 10 - avg_train_loss: 89.2206 avg_val_loss: 92.9723 MAE: 6.8923\n",
      "﹂ saving model with score: 6.8923\n",
      "Epoch 11 - avg_train_loss: 89.2164 avg_val_loss: 92.9672 MAE: 6.8920\n",
      "﹂ saving model with score: 6.8920\n",
      "Epoch 12 - avg_train_loss: 89.2130 avg_val_loss: 92.9634 MAE: 6.8917\n",
      "﹂ saving model with score: 6.8917\n",
      "Epoch 13 - avg_train_loss: 89.2102 avg_val_loss: 92.9600 MAE: 6.8915\n",
      "﹂ saving model with score: 6.8915\n",
      "Epoch 14 - avg_train_loss: 89.2079 avg_val_loss: 92.9570 MAE: 6.8913\n",
      "﹂ saving model with score: 6.8913\n",
      "Epoch 15 - avg_train_loss: 89.2060 avg_val_loss: 92.9549 MAE: 6.8911\n",
      "﹂ saving model with score: 6.8911\n",
      "Epoch 16 - avg_train_loss: 89.2045 avg_val_loss: 92.9531 MAE: 6.8910\n",
      "﹂ saving model with score: 6.8910\n",
      "Epoch 17 - avg_train_loss: 89.2032 avg_val_loss: 92.9516 MAE: 6.8909\n",
      "﹂ saving model with score: 6.8909\n",
      "Epoch 18 - avg_train_loss: 89.2021 avg_val_loss: 92.9506 MAE: 6.8908\n",
      "﹂ saving model with score: 6.8908\n",
      "Epoch 19 - avg_train_loss: 89.2013 avg_val_loss: 92.9497 MAE: 6.8907\n",
      "﹂ saving model with score: 6.8907\n",
      "Epoch 20 - avg_train_loss: 89.2005 avg_val_loss: 92.9490 MAE: 6.8906\n",
      "﹂ saving model with score: 6.8906\n",
      "Epoch 21 - avg_train_loss: 89.1999 avg_val_loss: 92.9485 MAE: 6.8906\n",
      "Epoch 22 - avg_train_loss: 89.1994 avg_val_loss: 92.9480 MAE: 6.8906\n",
      "Epoch 23 - avg_train_loss: 89.1990 avg_val_loss: 92.9478 MAE: 6.8905\n",
      "﹂ saving model with score: 6.8905\n",
      "Epoch 24 - avg_train_loss: 89.1986 avg_val_loss: 92.9476 MAE: 6.8905\n",
      "Epoch 25 - avg_train_loss: 89.1984 avg_val_loss: 92.9474 MAE: 6.8905\n",
      "Epoch 26 - avg_train_loss: 89.1981 avg_val_loss: 92.9472 MAE: 6.8905\n",
      "Epoch 27 - avg_train_loss: 89.1979 avg_val_loss: 92.9473 MAE: 6.8905\n",
      "Epoch 28 - avg_train_loss: 89.1977 avg_val_loss: 92.9473 MAE: 6.8905\n",
      "Epoch 29 - avg_train_loss: 89.1976 avg_val_loss: 92.9474 MAE: 6.8905\n",
      "Epoch 30 - avg_train_loss: 89.1975 avg_val_loss: 92.9474 MAE: 6.8905\n",
      "Epoch 31 - avg_train_loss: 89.1974 avg_val_loss: 92.9473 MAE: 6.8905\n",
      "Epoch 32 - avg_train_loss: 89.1973 avg_val_loss: 92.9473 MAE: 6.8905\n",
      "Epoch 33 - avg_train_loss: 89.1972 avg_val_loss: 92.9476 MAE: 6.8905\n",
      "Triggering Early Stopping on epoch 33\n",
      "----------------- Fold: 2 -----------------\n",
      "Epoch 1 - avg_train_loss: 89.6736 avg_val_loss: 115.1440 MAE: 7.5962\n",
      "﹂ saving model with score: 7.5962\n",
      "Epoch 2 - avg_train_loss: 89.6684 avg_val_loss: 115.1369 MAE: 7.5957\n",
      "﹂ saving model with score: 7.5957\n",
      "Epoch 3 - avg_train_loss: 89.6627 avg_val_loss: 115.1255 MAE: 7.5951\n",
      "﹂ saving model with score: 7.5951\n",
      "Epoch 4 - avg_train_loss: 89.6525 avg_val_loss: 115.1010 MAE: 7.5940\n",
      "﹂ saving model with score: 7.5940\n",
      "Epoch 5 - avg_train_loss: 89.6405 avg_val_loss: 115.0757 MAE: 7.5928\n",
      "﹂ saving model with score: 7.5928\n",
      "Epoch 6 - avg_train_loss: 89.6297 avg_val_loss: 115.0527 MAE: 7.5916\n",
      "﹂ saving model with score: 7.5916\n",
      "Epoch 7 - avg_train_loss: 89.6207 avg_val_loss: 115.0349 MAE: 7.5907\n",
      "﹂ saving model with score: 7.5907\n",
      "Epoch 8 - avg_train_loss: 89.6133 avg_val_loss: 115.0205 MAE: 7.5899\n",
      "﹂ saving model with score: 7.5899\n",
      "Epoch 9 - avg_train_loss: 89.6073 avg_val_loss: 115.0090 MAE: 7.5892\n",
      "﹂ saving model with score: 7.5892\n",
      "Epoch 10 - avg_train_loss: 89.6025 avg_val_loss: 115.0000 MAE: 7.5887\n",
      "﹂ saving model with score: 7.5887\n",
      "Epoch 11 - avg_train_loss: 89.5986 avg_val_loss: 114.9930 MAE: 7.5882\n",
      "﹂ saving model with score: 7.5882\n",
      "Epoch 12 - avg_train_loss: 89.5954 avg_val_loss: 114.9875 MAE: 7.5879\n",
      "﹂ saving model with score: 7.5879\n",
      "Epoch 13 - avg_train_loss: 89.5929 avg_val_loss: 114.9831 MAE: 7.5876\n",
      "﹂ saving model with score: 7.5876\n",
      "Epoch 14 - avg_train_loss: 89.5908 avg_val_loss: 114.9797 MAE: 7.5873\n",
      "﹂ saving model with score: 7.5873\n",
      "Epoch 15 - avg_train_loss: 89.5892 avg_val_loss: 114.9768 MAE: 7.5871\n",
      "﹂ saving model with score: 7.5871\n",
      "Epoch 16 - avg_train_loss: 89.5878 avg_val_loss: 114.9747 MAE: 7.5870\n",
      "﹂ saving model with score: 7.5870\n",
      "Epoch 17 - avg_train_loss: 89.5867 avg_val_loss: 114.9731 MAE: 7.5868\n",
      "﹂ saving model with score: 7.5868\n",
      "Epoch 18 - avg_train_loss: 89.5857 avg_val_loss: 114.9718 MAE: 7.5867\n",
      "﹂ saving model with score: 7.5867\n",
      "Epoch 19 - avg_train_loss: 89.5850 avg_val_loss: 114.9710 MAE: 7.5866\n",
      "﹂ saving model with score: 7.5866\n",
      "Epoch 20 - avg_train_loss: 89.5843 avg_val_loss: 114.9701 MAE: 7.5865\n",
      "﹂ saving model with score: 7.5865\n",
      "Epoch 21 - avg_train_loss: 89.5838 avg_val_loss: 114.9695 MAE: 7.5865\n",
      "Epoch 22 - avg_train_loss: 89.5834 avg_val_loss: 114.9691 MAE: 7.5864\n",
      "﹂ saving model with score: 7.5864\n",
      "Epoch 23 - avg_train_loss: 89.5830 avg_val_loss: 114.9688 MAE: 7.5864\n",
      "Epoch 24 - avg_train_loss: 89.5827 avg_val_loss: 114.9686 MAE: 7.5863\n",
      "﹂ saving model with score: 7.5863\n",
      "Epoch 25 - avg_train_loss: 89.5824 avg_val_loss: 114.9685 MAE: 7.5863\n",
      "Epoch 26 - avg_train_loss: 89.5822 avg_val_loss: 114.9684 MAE: 7.5863\n",
      "Epoch 27 - avg_train_loss: 89.5821 avg_val_loss: 114.9684 MAE: 7.5862\n",
      "﹂ saving model with score: 7.5862\n",
      "Epoch 28 - avg_train_loss: 89.5819 avg_val_loss: 114.9684 MAE: 7.5862\n",
      "Epoch 29 - avg_train_loss: 89.5818 avg_val_loss: 114.9684 MAE: 7.5862\n",
      "Epoch 30 - avg_train_loss: 89.5817 avg_val_loss: 114.9685 MAE: 7.5862\n",
      "Epoch 31 - avg_train_loss: 89.5816 avg_val_loss: 114.9685 MAE: 7.5862\n",
      "Epoch 32 - avg_train_loss: 89.5815 avg_val_loss: 114.9686 MAE: 7.5861\n",
      "Epoch 33 - avg_train_loss: 89.5814 avg_val_loss: 114.9687 MAE: 7.5861\n",
      "﹂ saving model with score: 7.5861\n",
      "Epoch 34 - avg_train_loss: 89.5814 avg_val_loss: 114.9688 MAE: 7.5861\n",
      "Epoch 35 - avg_train_loss: 89.5813 avg_val_loss: 114.9689 MAE: 7.5861\n",
      "Epoch 36 - avg_train_loss: 89.5813 avg_val_loss: 114.9690 MAE: 7.5861\n",
      "Epoch 37 - avg_train_loss: 89.5813 avg_val_loss: 114.9691 MAE: 7.5861\n",
      "Epoch 38 - avg_train_loss: 89.5813 avg_val_loss: 114.9692 MAE: 7.5861\n",
      "Epoch 39 - avg_train_loss: 89.5812 avg_val_loss: 114.9693 MAE: 7.5861\n",
      "Epoch 40 - avg_train_loss: 89.5812 avg_val_loss: 114.9694 MAE: 7.5861\n",
      "Epoch 41 - avg_train_loss: 89.5812 avg_val_loss: 114.9695 MAE: 7.5861\n",
      "Epoch 42 - avg_train_loss: 89.5812 avg_val_loss: 114.9695 MAE: 7.5861\n",
      "Epoch 43 - avg_train_loss: 89.5812 avg_val_loss: 114.9696 MAE: 7.5861\n",
      "Triggering Early Stopping on epoch 43\n",
      "----------------- Fold: 3 -----------------\n",
      "Epoch 1 - avg_train_loss: 90.6580 avg_val_loss: 184.2021 MAE: 9.0136\n",
      "﹂ saving model with score: 9.0136\n",
      "Epoch 2 - avg_train_loss: 90.6508 avg_val_loss: 184.2996 MAE: 9.0170\n",
      "Epoch 3 - avg_train_loss: 90.6412 avg_val_loss: 184.3806 MAE: 9.0207\n",
      "Epoch 4 - avg_train_loss: 90.6295 avg_val_loss: 184.4046 MAE: 9.0217\n",
      "Epoch 5 - avg_train_loss: 90.6191 avg_val_loss: 184.4041 MAE: 9.0218\n",
      "Epoch 6 - avg_train_loss: 90.6101 avg_val_loss: 184.3994 MAE: 9.0216\n",
      "Epoch 7 - avg_train_loss: 90.6026 avg_val_loss: 184.3874 MAE: 9.0210\n",
      "Epoch 8 - avg_train_loss: 90.5963 avg_val_loss: 184.3729 MAE: 9.0203\n",
      "Epoch 9 - avg_train_loss: 90.5912 avg_val_loss: 184.3566 MAE: 9.0196\n",
      "Epoch 10 - avg_train_loss: 90.5869 avg_val_loss: 184.3402 MAE: 9.0188\n",
      "Epoch 11 - avg_train_loss: 90.5833 avg_val_loss: 184.3243 MAE: 9.0181\n",
      "Triggering Early Stopping on epoch 11\n",
      "----------------- Fold: 4 -----------------\n",
      "Epoch 1 - avg_train_loss: 93.1388 avg_val_loss: 182.0200 MAE: 8.7863\n",
      "﹂ saving model with score: 8.7863\n",
      "Epoch 2 - avg_train_loss: 93.1312 avg_val_loss: 182.0060 MAE: 8.7858\n",
      "﹂ saving model with score: 8.7858\n",
      "Epoch 3 - avg_train_loss: 93.1197 avg_val_loss: 181.9697 MAE: 8.7845\n",
      "﹂ saving model with score: 8.7845\n",
      "Epoch 4 - avg_train_loss: 93.1041 avg_val_loss: 181.9186 MAE: 8.7827\n",
      "﹂ saving model with score: 8.7827\n",
      "Epoch 5 - avg_train_loss: 93.0899 avg_val_loss: 181.8724 MAE: 8.7811\n",
      "﹂ saving model with score: 8.7811\n",
      "Epoch 6 - avg_train_loss: 93.0790 avg_val_loss: 181.8365 MAE: 8.7799\n",
      "﹂ saving model with score: 8.7799\n",
      "Epoch 7 - avg_train_loss: 93.0709 avg_val_loss: 181.8085 MAE: 8.7789\n",
      "﹂ saving model with score: 8.7789\n",
      "Epoch 8 - avg_train_loss: 93.0648 avg_val_loss: 181.7864 MAE: 8.7781\n",
      "﹂ saving model with score: 8.7781\n",
      "Epoch 9 - avg_train_loss: 93.0600 avg_val_loss: 181.7688 MAE: 8.7775\n",
      "﹂ saving model with score: 8.7775\n",
      "Epoch 10 - avg_train_loss: 93.0563 avg_val_loss: 181.7548 MAE: 8.7770\n",
      "﹂ saving model with score: 8.7770\n",
      "Epoch 11 - avg_train_loss: 93.0535 avg_val_loss: 181.7435 MAE: 8.7766\n",
      "﹂ saving model with score: 8.7766\n",
      "Epoch 12 - avg_train_loss: 93.0513 avg_val_loss: 181.7343 MAE: 8.7763\n",
      "﹂ saving model with score: 8.7763\n",
      "Epoch 13 - avg_train_loss: 93.0496 avg_val_loss: 181.7267 MAE: 8.7761\n",
      "﹂ saving model with score: 8.7761\n",
      "Epoch 14 - avg_train_loss: 93.0482 avg_val_loss: 181.7205 MAE: 8.7759\n",
      "﹂ saving model with score: 8.7759\n",
      "Epoch 15 - avg_train_loss: 93.0471 avg_val_loss: 181.7153 MAE: 8.7757\n",
      "﹂ saving model with score: 8.7757\n",
      "Epoch 16 - avg_train_loss: 93.0462 avg_val_loss: 181.7110 MAE: 8.7755\n",
      "﹂ saving model with score: 8.7755\n",
      "Epoch 17 - avg_train_loss: 93.0455 avg_val_loss: 181.7073 MAE: 8.7754\n",
      "﹂ saving model with score: 8.7754\n",
      "Epoch 18 - avg_train_loss: 93.0450 avg_val_loss: 181.7042 MAE: 8.7753\n",
      "﹂ saving model with score: 8.7753\n",
      "Epoch 19 - avg_train_loss: 93.0445 avg_val_loss: 181.7016 MAE: 8.7753\n",
      "Epoch 20 - avg_train_loss: 93.0442 avg_val_loss: 181.6994 MAE: 8.7752\n",
      "﹂ saving model with score: 8.7752\n",
      "Epoch 21 - avg_train_loss: 93.0439 avg_val_loss: 181.6975 MAE: 8.7751\n",
      "﹂ saving model with score: 8.7751\n",
      "Epoch 22 - avg_train_loss: 93.0437 avg_val_loss: 181.6958 MAE: 8.7751\n",
      "Epoch 23 - avg_train_loss: 93.0435 avg_val_loss: 181.6944 MAE: 8.7750\n",
      "﹂ saving model with score: 8.7750\n",
      "Epoch 24 - avg_train_loss: 93.0433 avg_val_loss: 181.6931 MAE: 8.7750\n",
      "Epoch 25 - avg_train_loss: 93.0432 avg_val_loss: 181.6921 MAE: 8.7750\n",
      "Epoch 26 - avg_train_loss: 93.0431 avg_val_loss: 181.6911 MAE: 8.7750\n",
      "Epoch 27 - avg_train_loss: 93.0430 avg_val_loss: 181.6903 MAE: 8.7749\n",
      "﹂ saving model with score: 8.7749\n",
      "Epoch 28 - avg_train_loss: 93.0429 avg_val_loss: 181.6895 MAE: 8.7749\n",
      "Epoch 29 - avg_train_loss: 93.0429 avg_val_loss: 181.6889 MAE: 8.7749\n",
      "Epoch 30 - avg_train_loss: 93.0428 avg_val_loss: 181.6883 MAE: 8.7749\n",
      "Epoch 31 - avg_train_loss: 93.0428 avg_val_loss: 181.6878 MAE: 8.7749\n",
      "Epoch 32 - avg_train_loss: 93.0427 avg_val_loss: 181.6874 MAE: 8.7749\n",
      "Epoch 33 - avg_train_loss: 93.0427 avg_val_loss: 181.6870 MAE: 8.7748\n",
      "﹂ saving model with score: 8.7748\n",
      "Epoch 34 - avg_train_loss: 93.0427 avg_val_loss: 181.6866 MAE: 8.7748\n",
      "Epoch 35 - avg_train_loss: 93.0426 avg_val_loss: 181.6863 MAE: 8.7748\n",
      "Epoch 36 - avg_train_loss: 93.0426 avg_val_loss: 181.6860 MAE: 8.7748\n",
      "Epoch 37 - avg_train_loss: 93.0426 avg_val_loss: 181.6857 MAE: 8.7748\n",
      "Epoch 38 - avg_train_loss: 93.0426 avg_val_loss: 181.6854 MAE: 8.7748\n",
      "Epoch 39 - avg_train_loss: 93.0426 avg_val_loss: 181.6852 MAE: 8.7748\n",
      "Epoch 40 - avg_train_loss: 93.0426 avg_val_loss: 181.6850 MAE: 8.7748\n",
      "Epoch 41 - avg_train_loss: 93.0425 avg_val_loss: 181.6848 MAE: 8.7748\n",
      "Epoch 42 - avg_train_loss: 93.0425 avg_val_loss: 181.6847 MAE: 8.7748\n",
      "Epoch 43 - avg_train_loss: 93.0425 avg_val_loss: 181.6845 MAE: 8.7748\n",
      "Triggering Early Stopping on epoch 43\n",
      "----------------- Fold: 5 -----------------\n",
      "Epoch 1 - avg_train_loss: 95.5825 avg_val_loss: 114.7895 MAE: 7.4282\n",
      "﹂ saving model with score: 7.4282\n",
      "Epoch 2 - avg_train_loss: 95.5757 avg_val_loss: 114.7755 MAE: 7.4274\n",
      "﹂ saving model with score: 7.4274\n",
      "Epoch 3 - avg_train_loss: 95.5677 avg_val_loss: 114.7405 MAE: 7.4255\n",
      "﹂ saving model with score: 7.4255\n",
      "Epoch 4 - avg_train_loss: 95.5545 avg_val_loss: 114.7017 MAE: 7.4233\n",
      "﹂ saving model with score: 7.4233\n",
      "Epoch 5 - avg_train_loss: 95.5409 avg_val_loss: 114.6769 MAE: 7.4217\n",
      "﹂ saving model with score: 7.4217\n",
      "Epoch 6 - avg_train_loss: 95.5287 avg_val_loss: 114.6648 MAE: 7.4207\n",
      "﹂ saving model with score: 7.4207\n",
      "Epoch 7 - avg_train_loss: 95.5190 avg_val_loss: 114.6539 MAE: 7.4198\n",
      "﹂ saving model with score: 7.4198\n",
      "Epoch 8 - avg_train_loss: 95.5107 avg_val_loss: 114.6503 MAE: 7.4193\n",
      "﹂ saving model with score: 7.4193\n",
      "Epoch 9 - avg_train_loss: 95.5043 avg_val_loss: 114.6445 MAE: 7.4188\n",
      "﹂ saving model with score: 7.4188\n",
      "Epoch 10 - avg_train_loss: 95.4987 avg_val_loss: 114.6437 MAE: 7.4185\n",
      "﹂ saving model with score: 7.4185\n",
      "Epoch 11 - avg_train_loss: 95.4945 avg_val_loss: 114.6404 MAE: 7.4182\n",
      "﹂ saving model with score: 7.4182\n",
      "Epoch 12 - avg_train_loss: 95.4908 avg_val_loss: 114.6411 MAE: 7.4180\n",
      "﹂ saving model with score: 7.4180\n",
      "Epoch 13 - avg_train_loss: 95.4881 avg_val_loss: 114.6387 MAE: 7.4178\n",
      "﹂ saving model with score: 7.4178\n",
      "Epoch 14 - avg_train_loss: 95.4856 avg_val_loss: 114.6386 MAE: 7.4177\n",
      "﹂ saving model with score: 7.4177\n",
      "Epoch 15 - avg_train_loss: 95.4836 avg_val_loss: 114.6386 MAE: 7.4176\n",
      "﹂ saving model with score: 7.4176\n",
      "Epoch 16 - avg_train_loss: 95.4819 avg_val_loss: 114.6406 MAE: 7.4175\n",
      "﹂ saving model with score: 7.4175\n",
      "Epoch 17 - avg_train_loss: 95.4809 avg_val_loss: 114.6387 MAE: 7.4174\n",
      "﹂ saving model with score: 7.4174\n",
      "Epoch 18 - avg_train_loss: 95.4795 avg_val_loss: 114.6405 MAE: 7.4174\n",
      "Epoch 19 - avg_train_loss: 95.4789 avg_val_loss: 114.6390 MAE: 7.4173\n",
      "﹂ saving model with score: 7.4173\n",
      "Epoch 20 - avg_train_loss: 95.4779 avg_val_loss: 114.6395 MAE: 7.4173\n",
      "Epoch 21 - avg_train_loss: 95.4772 avg_val_loss: 114.6415 MAE: 7.4173\n",
      "Epoch 22 - avg_train_loss: 95.4770 avg_val_loss: 114.6398 MAE: 7.4172\n",
      "﹂ saving model with score: 7.4172\n",
      "Epoch 23 - avg_train_loss: 95.4763 avg_val_loss: 114.6403 MAE: 7.4172\n",
      "Epoch 24 - avg_train_loss: 95.4759 avg_val_loss: 114.6407 MAE: 7.4172\n",
      "Epoch 25 - avg_train_loss: 95.4756 avg_val_loss: 114.6411 MAE: 7.4172\n",
      "Epoch 26 - avg_train_loss: 95.4753 avg_val_loss: 114.6413 MAE: 7.4172\n",
      "Epoch 27 - avg_train_loss: 95.4751 avg_val_loss: 114.6416 MAE: 7.4172\n",
      "Epoch 28 - avg_train_loss: 95.4750 avg_val_loss: 114.6423 MAE: 7.4172\n",
      "Epoch 29 - avg_train_loss: 95.4747 avg_val_loss: 114.6424 MAE: 7.4172\n",
      "Epoch 30 - avg_train_loss: 95.4746 avg_val_loss: 114.6424 MAE: 7.4172\n",
      "Epoch 31 - avg_train_loss: 95.4745 avg_val_loss: 114.6425 MAE: 7.4172\n",
      "Epoch 32 - avg_train_loss: 95.4744 avg_val_loss: 114.6426 MAE: 7.4172\n",
      "Triggering Early Stopping on epoch 32\n",
      "Average MAE across folds: 7.936477851867676\n"
     ]
    }
   ],
   "source": [
    "def train(df):\n",
    "    Fold = PurgedGroupTimeSeriesSplit(n_splits=CFG.N_FOLDS,\n",
    "                                      max_train_group_size=10000,\n",
    "                                      max_val_group_size=200,\n",
    "                                      val_group_gap=10)\n",
    "    scores = np.empty([CFG.N_FOLDS])\n",
    "    groups = df['time_id']\n",
    "    for fold, (train_index, val_index) in enumerate(Fold.split(df, df[CFG.TARGET_COLS], groups=groups)):\n",
    "        train = df.iloc[train_index].reset_index(drop=True)\n",
    "        val = df.iloc[val_index].reset_index(drop=True)\n",
    "        score = train_loop(train, val, fold)\n",
    "        scores[fold] = score\n",
    "    print(f'Average MAE across folds: {np.mean(scores)}')\n",
    "         \n",
    "train(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a05f6373",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T09:35:32.317847Z",
     "iopub.status.busy": "2023-10-20T09:35:32.317174Z",
     "iopub.status.idle": "2023-10-20T09:35:32.326670Z",
     "shell.execute_reply": "2023-10-20T09:35:32.325433Z"
    },
    "papermill": {
     "duration": 0.070275,
     "end_time": "2023-10-20T09:35:32.328802",
     "exception": false,
     "start_time": "2023-10-20T09:35:32.258527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    models = []\n",
    "    for i in range(CFG.N_FOLDS):   \n",
    "        model = CNN()\n",
    "        model.load_state_dict(torch.load(f'/home/fraverta/development/ML/Project/model_fold_{i+1}'))\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "def inference(models, y_test):\n",
    "    preds = []\n",
    "    for i in range(len(models)):\n",
    "        print(f'Inferencing model {i+1}')\n",
    "        pred = models[i](y_test)[0]\n",
    "        preds.append(pred.detach().numpy())\n",
    "    return np.mean(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "450bba01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n",
      "Index(['stock_id', 'date_id', 'seconds_in_bucket', 'imbalance_size',\n",
      "       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
      "       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n",
      "       'ask_size', 'wap', 'target', 'time_id', 'row_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('trading_the_close_data/example_test_files/test.csv')\n",
    "df_test_targets = pd.read_csv('trading_the_close_data/example_test_files/revealed_targets.csv')[['stock_id', 'date_id', 'seconds_in_bucket', 'revealed_target']]\n",
    "\n",
    "# merge df_test and df_test_targets based on stock_id date_id\tseconds_in_bucket\n",
    "df_test = df_test.merge(df_test_targets, on=['stock_id', 'date_id', 'seconds_in_bucket'], how='inner')\n",
    "# select all  columns except revealed_target\n",
    "df_test, y_test = df_test[df_test.columns.difference(['revealed_target'])] , df_test['revealed_target']\n",
    "df_test['target'] = y_test\n",
    "df_test.drop(['currently_scored'], axis=1, inplace=True)\n",
    "df_test = df_test\n",
    "\n",
    "print(len(df_test))\n",
    "\n",
    "cols_during_training = ['stock_id', 'date_id', 'seconds_in_bucket', 'imbalance_size',\n",
    "       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
    "       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n",
    "       'ask_size', 'wap', 'target', 'time_id', 'row_id']\n",
    "\n",
    "assert not [c for c in df_test.columns if c not in cols_during_training]\n",
    "assert not [c for c in cols_during_training if c not in df_test.columns]\n",
    "\n",
    "# sort df_tes columns as C\n",
    "df_test = df_test[cols_during_training]\n",
    "\n",
    "\n",
    "\n",
    "new_df_processed = preprocess(df_test, mode='test')\n",
    "new_dataset = get_dataset(new_df_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9da8541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_models(models, dataset):\n",
    "    dataloader = DataLoader(dataset, batch_size=len(new_dataset), shuffle=False)\n",
    "    all_predictions = []\n",
    "    for model in models:\n",
    "        model_predictions = []\n",
    "        for inputs, _ in dataloader:  # No labels in prediction\n",
    "            inputs = inputs.to(device)  # Move inputs to the same device as model\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                model_predictions.append(outputs.cpu().numpy())\n",
    "        all_predictions.append(np.concatenate(model_predictions, axis=0))\n",
    "    # Average predictions across models\n",
    "    avg_predictions = np.mean(all_predictions, axis=0)\n",
    "    return avg_predictions\n",
    "\n",
    "models = get_models()\n",
    "preds = predict_with_models(models, new_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fa9c1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0028205246\n",
      "MAE 5.404275962407698\n",
      "Sign Accuracy 0.48984848484848487\n"
     ]
    }
   ],
   "source": [
    "preds = preds.flatten()\n",
    "print(preds[0])\n",
    "print(\"MAE\", abs(preds - y_test).mean())\n",
    "\n",
    "\n",
    "# count the number of predictions that were same sign\n",
    "same_sign = np.sign(preds) == np.sign(y_test)\n",
    "print(\"Sign Accuracy\", same_sign.sum() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9addc087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df from preds and y_test\n",
    "df = pd.DataFrame({'preds': preds, 'y_test': y_test})\n",
    "df.to_csv('preds.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2800.264156,
   "end_time": "2023-10-20T09:37:16.144228",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-20T08:50:35.880072",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
